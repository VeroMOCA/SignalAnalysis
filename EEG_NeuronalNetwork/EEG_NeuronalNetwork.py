# -*- coding: utf-8 -*-
"""Proyecto Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16GVeSFTv-5lsYNJ6D8zg87QAhUhgQSyL

Comparación de uso de PCA e ICA para la clasificación de electroencefalografía con redes neuronales recurrentes y perceptron multicapa.
"""

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D

from sklearn.preprocessing import LabelEncoder, StandardScaler

from sklearn.decomposition import FastICA
from sklearn.decomposition import PCA

from sklearn.model_selection import train_test_split

from sklearn.metrics import confusion_matrix, classification_report

import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Dense, Conv2D, Dropout, MaxPooling2D, Flatten, Input,LSTM
from tensorflow.keras.models import Model
from keras.layers import Bidirectional

#Cargamos los 4 datasets que se utilizarán, es uno por paciente
df_a = pd.read_csv('/content/user_a.csv')
df_b = pd.read_csv('/content/user_b.csv')
df_c = pd.read_csv('/content/user_c.csv')
df_d = pd.read_csv('/content/user_d.csv')
datasets = [df_a, df_b, df_c, df_d] #Se guardan todos en un arreglo para luego poder hacer procesos iterativos



"""# Funciones"""

#Se realiza el test_split para cada dataset
def division_Dataset(datasets, data_y,paciente):
  X_train, X_test, y_train, y_test = train_test_split(datasets, data_y, train_size=0.7, random_state=123)
  standard = StandardScaler().fit(X_train)
  X_train = standard.transform(X_train)
  X_test = standard.transform(X_test)
  print("Se realizo la division de datasets para el paciente ", paciente)
  return X_train, X_test, y_train, y_test

#Función para graficar la matriz de confusión
def graficar(cm):
  plt.figure(figsize = (8,8))
  sns.heatmap (cm, annot=True, vmin=0, fmt='g', cbar=False, cmap='Blues')
  plt.xticks(np.arange(3)+0.5, label_mapping.keys())
  plt.yticks(np.arange(3)+0.5, label_mapping.keys())
  plt.xlabel("Predicted")
  plt.ylabel("Actual")
  plt.title("Confusion Matrix")
  plt.show()

#Con esta función se realiza cada PCA e ICA establecidos
def preprocesamiento(dataset, modelo):
  modelo.fit(dataset)
  models = modelo.transform(dataset)
  #print(len(models))
  return models

#Esta función simplementa da un formato al dataset para poder ser manipulable en los siguientes pasos
#Separa los datos de las etiquetas, los pasa a tipo DataFrame y los pasa a su transpuesta
def transformando_datasets(datasets):
  target = 'Class'
  col = datasets[0].columns
  features = col[1:-1]
  datasets_transform = []
  datasets_y = []
  for i in range(0,len(datasets)):
    data_transform = [datasets[i][features[j]] for j in range(0,len(features))]
    data_transform = pd.DataFrame(data_transform).T
    datasets_transform.append(data_transform)
    y= np.array(datasets[i]['Class'])
    datasets_y.append(y)
  return datasets_transform, datasets_y

def red_recurrente(X_train, X_test, y_train, y_test, paciente):
  #Definimos el modelo de la red neuronal recurrente
  inputs = tf.keras.Input(shape = (X_train.shape[1],))
  expand_dims = tf.expand_dims(inputs, axis=2)
  gru = tf.keras.layers.GRU(256, return_sequences=True)(expand_dims)
  flatten = tf.keras.layers.Flatten()(gru)
  outputs = tf.keras.layers.Dense(3, activation = 'softmax')(flatten)
  model_rnn = tf.keras.Model(inputs=inputs, outputs=outputs)

  #Pasamos los parametros para compilar la red neuronal
  print("Evaluamos la red neuronal recurrente (GRU) para el paciente", paciente)
  model_rnn.compile(
    optimizer =  'adam',
    loss = 'sparse_categorical_crossentropy',
    metrics = ['accuracy']
  )

  #Entrenamos la red neuronal, para validar los resultados se genera un split del 20% durante el entrenamiento
  print()
  history = model_rnn.fit(
      X_train,
      y_train,
      validation_split = 0.2,
      batch_size = 32,
      epochs=50,
      verbose = 0,
      callbacks=[
          tf.keras.callbacks.EarlyStopping(
              monitor = 'val_loss',
              patience =5,
              restore_best_weights=True
          )
      ]
  )
  #Evaluamos para los datos guardados en test
  model_acc = model_rnn.evaluate(X_test, y_test, verbose=0)[1]
  print("Test Accuracy_ {:.3f}%".format(model_acc * 100))

  #Graficamos los resultados de la prueba
  y_pred = np.array(list(map(lambda x: np.argmax(x), model_rnn.predict(X_test))))
  cm = confusion_matrix(y_test, y_pred)
  clr = classification_report(y_test, y_pred, target_names = label_mapping.keys())
  print("El reporte de clasificación es: \n----------------------------\n", clr)
  graficar(cm)

def red_no_recurrente(X_train, X_test, y_train, y_test, paciente):
  print("\n\n\n-----------------\n")
  #Definimos el modelo de la red neuronal neuronal que es el modelo estandar
  inputs = tf.keras.Input(shape = (X_train.shape[1],))
  x = tf.keras.layers.Dense(128,activation='relu')(inputs)
  x=tf.keras.layers.Dense(128, activation='relu')(x)
  outputs = tf.keras.layers.Dense(3, activation = 'softmax')(x)
  model_no_rnn = tf.keras.Model(inputs= inputs, outputs=outputs)

  #Pasamos los parametros para compilar la red neuronal
  print("Evaluamos el modelo estandar para el paciente", paciente)
  model_no_rnn.compile(
    optimizer =  'adam',
    loss = 'sparse_categorical_crossentropy',
    metrics = ['accuracy']
  )

  #Entrenamos la red neuronal, para validar los resultados se genera un split del 20% durante el entrenamiento
  print()
  history = model_no_rnn.fit(
      X_train,
      y_train,
      validation_split = 0.2,
      batch_size = 32,
      epochs=50,
      verbose = 0,
      callbacks=[
          tf.keras.callbacks.EarlyStopping(
              monitor = 'val_loss',
              patience =5,
              restore_best_weights=True
          )
      ]
  )
  #Evaluamos para los datos guardados en test
  model_acc = model_no_rnn.evaluate(X_test, y_test, verbose=0)[1]
  print("Test Accuracy_ {:.3f}%".format(model_acc * 100))

  #Graficamos los resultados de la prueba
  y_pred = np.array(list(map(lambda x: np.argmax(x), model_no_rnn.predict(X_test))))
  cm = confusion_matrix(y_test, y_pred)
  clr = classification_report(y_test, y_pred, target_names = label_mapping.keys())
  print("El reporte de clasificación es: \n----------------------------\n", clr)
  graficar(cm)

def red_no_recurrente2(X_train, X_test, y_train, y_test, paciente):
  print("\n\n\n-----------------\n")
  #Definimos el modelo de la red neuronal perceptron multicapa
  inputs = tf.keras.Input(shape = (X_train.shape[1],))
  x = tf.keras.layers.Dense(256,activation='relu')(inputs)
  x=tf.keras.layers.Dense(128, activation='relu')(x)
  x=tf.keras.layers.Dense(64, activation='relu')(x)
  x=tf.keras.layers.Dense(32, activation='relu')(x)
  outputs = tf.keras.layers.Dense(3, activation = 'softmax')(x)
  model_no_rnn2 = tf.keras.Model(inputs= inputs, outputs=outputs)

  #Pasamos los parametros para compilar la red neuronal
  print("Evaluamos el perceptron multicapa 2 para el paciente", paciente)
  model_no_rnn2.compile(
    optimizer =  'adam',
    loss = 'sparse_categorical_crossentropy',
    metrics = ['accuracy']
  )

  #Entrenamos la red neuronal, para validar los resultados se genera un split del 20% durante el entrenamiento
  history = model_no_rnn2.fit(
      X_train,
      y_train,
      validation_split = 0.2,
      batch_size = 32,
      epochs=50,
      verbose = 0,
      callbacks=[
          tf.keras.callbacks.EarlyStopping(
              monitor = 'val_loss',
              patience =5,
              restore_best_weights=True
          )
      ]
  )
  #Evaluamos para los datos guardados en test
  model_acc = model_no_rnn2.evaluate(X_test, y_test, verbose=0)[1]
  print("Test Accuracy_ {:.3f}%".format(model_acc * 100))

  #Graficamos los resultados de la prueba
  y_pred = np.array(list(map(lambda x: np.argmax(x), model_no_rnn2.predict(X_test))))
  cm = confusion_matrix(y_test, y_pred)
  clr = classification_report(y_test, y_pred, target_names = label_mapping.keys())
  print("El reporte de clasificación es: \n----------------------------\n", clr)
  graficar(cm)

def evaluacion(datasets, data_y, feature_extraction, paciente):
  #Realizamos el test_split para cada paciente
  X_train, X_test, y_train, y_test = division_Dataset(datasets, data_y,  paciente)
  #print(y_test.shape)
  #Evaluamos para el modelo de la red neuronal recurrente
  red_recurrente(X_train, X_test, y_train, y_test, paciente)
  #Evaluamos para el modelo de la red neuronal no recurrente
  red_no_recurrente(X_train, X_test, y_train, y_test, paciente)
  #Evaluamos para el modelo de la red neuronal no recurrente
  red_no_recurrente2(X_train, X_test, y_train, y_test, paciente)

"""# Variables"""

#Label mapping de las etiquetas, nos servirá para hacer la matriz de confusión posteriormente
label_mapping = {'Negative':0, 'Right': 1, 'Left':2}

#Creamos diferentes listas para todos los datos que se ocuparan
#Nombre de los pacientes
Pacientes = [
    "Paciente a",
    "Paciente b",
    "Paciente c",
    "Paciente d"
]

#Definición de los métodos de feature extraction que se utilizaran
feature_extraction= [
    PCA(n_components=10),
    FastICA(n_components=10)
]
#Nombres de todos los métodos de feature extraction que se utilizaran
feature_extraction_names= [
    "PCA_10",
    "ICA_10"
]

#Separamos el dataset entre valores de entrada y etiquetas.
datasets_transform, datasets_y = transformando_datasets(datasets)

#Se realiza el feature extraction para cada paciente en todos los métodos utilizados
for i in range (len(feature_extraction)): #llamamos método por método de feature extraction
  clr = feature_extraction[i] #se crea un modelo para cada metodo
  print("Método de feature extraction", feature_extraction_names[i])
  for j in range (len(datasets_transform)): #llamamos paciente a paciente por cada método
    models = preprocesamiento(datasets_transform[j], clr)
    print("Se realizo el feature extraction para el paciente:", Pacientes[j])
    evaluacion(models, datasets_y[j],feature_extraction_names[i], Pacientes[j]) #Se evalua todas las redes para cada método